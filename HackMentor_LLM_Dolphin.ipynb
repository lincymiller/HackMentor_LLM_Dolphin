{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNn2fxPK8BmM4jojnT2JgYr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIXFtrBnotyA",
        "outputId": "cfed1d9b-c62e-41f2-b7d6-644015143a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nrTazSjgvvP",
        "outputId": "717c9ff7-48f5-49b5-d0cd-35d3e51e4cf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the folder in your Google Drive\n",
        "folder_path = '/content/drive/MyDrive/HackMentor LLM Dolphin (Models-pretrained)'\n",
        "\n",
        "# List files in the specified folder\n",
        "files = os.listdir(folder_path)\n",
        "print(\"Files in folder:\", files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyn3iuF_hzpT",
        "outputId": "a8439619-f77e-47ae-a1fe-84ce0a985a9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in folder: ['model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Specify paths\n",
        "source_folder = '/content/drive/MyDrive/HackMentor LLM Dolphin (Models-pretrained)'\n",
        "destination_folder = '/content/your_model_directory'  # Change to match the directory where your Colab-uploaded files are\n",
        "\n",
        "# Copy large files from Google Drive to Colab\n",
        "shutil.copy(f'{source_folder}/model-00001-of-00004.safetensors', destination_folder)\n",
        "shutil.copy(f'{source_folder}/model-00002-of-00004.safetensors', destination_folder)\n",
        "shutil.copy(f'{source_folder}/model-00003-of-00004.safetensors', destination_folder)\n"
      ],
      "metadata": {
        "id": "Yj5BoX8TkvvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f63da14d-98ba-4bb9-e79e-2378704cd0ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/your_model_directory'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir('/content/drive/MyDrive/HackMentor LLM Dolphin (Models-pretrained)')\n",
        "print(\"Files in the directory:\", files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iva-M3t2oLG9",
        "outputId": "f7b5e2fa-1c66-4e38-8e1e-a40e18f34b22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the directory: ['model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the destination path for model files\n",
        "destination_folder = '/content/your_model_directory'  # Update this path to your actual model directory\n",
        "\n",
        "# Check if the directory exists\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)  # Create only if it doesn't exist\n",
        "\n",
        "# Proceed with copying files or other tasks\n"
      ],
      "metadata": {
        "id": "KN3r2YdVq7l4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Update with the actual directory where your files are located\n",
        "destination_folder = '/content/HackMentor_LLM_Dolphin'\n",
        "files = os.listdir(destination_folder)\n",
        "print(\"Files in the destination folder:\", files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YzAHI9Cr60Y",
        "outputId": "a839aa8d-29fe-4605-f3c0-63cf722e65cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the destination folder: ['generation_config.json', 'model-00004-of-00004.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'adapter_config.json', 'tokenizer.json', 'adapter_model.bin', 'model.safetensors.index.json', 'config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Update with the actual directory where your files are located\n",
        "destination_folder = '/content/HackMentor_LLM_Dolphin'\n",
        "files = os.listdir(destination_folder)\n",
        "print(\"Files in the destination folder:\", files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaT8lsqht0rb",
        "outputId": "22be3987-0b98-446f-b113-ca1b70ee43a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the destination folder: ['generation_config.json', 'model-00004-of-00004.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'adapter_config.json', 'tokenizer.json', 'adapter_model.bin', 'model.safetensors.index.json', 'config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model_path = '/content/HackMentor_LLM_Dolphin'\n",
        "\n",
        "# Load configuration separately and ensure it doesn't reference any incorrect paths\n",
        "config = AutoConfig.from_pretrained(base_model_path, local_files_only=True)\n",
        "\n",
        "# Modify config directly in case there are path references (just to ensure nothing points elsewhere)\n",
        "config._name_or_path = base_model_path  # Set this to point explicitly to your directory, even though it may not fully eliminate the issue\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, local_files_only=True)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "try:\n",
        "    # Load model using the customized configuration\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_pTPP4rx5wG",
        "outputId": "24d93dc2-0417-4805-f585-e6f111135ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Tokenizer loaded successfully.\n",
            "Loading model...\n"
          ]
        }
      ]
    }
  ]
}